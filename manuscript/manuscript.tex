\documentclass[12pt]{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{natbib}

\title{Physics-Informed Neural Networks for Beam Deflection Analysis: Methodology and Applications}
\author{}
\date{}

\begin{document}

\maketitle

\begin{abstract}
\noindent This paper presents an advanced Physics-Informed Neural Network (PINN) methodology for structural beam analysis, introducing three key methodological improvements that address computational challenges in mechanics. We develop: (1) A hard-constrained architecture through output transformation $w_{\theta}(x) = x(1-x)\cdot\textrm{NN}(x)$ that enforces boundary conditions exactly for fourth-order systems; (2) A dynamic weighting scheme $w_{\text{BC}}(t)=10\cdot\exp(-0.0001t)$ that adaptively balances boundary constraints and PDE residuals during training, improving convergence efficiency; and (3) A regularized formulation using Gaussian approximation ($\sigma = 0.01L$) for Dirac delta loads that handles singularities without domain decomposition. Validated on cantilever, fully-restrained, and point-loaded beams, our approach achieves high accuracy ($\mathcal{O}(10^{-10})$ loss) with 0.56\% relative L2 error for concentrated mid-span loads. The method demonstrates computational advantages including mesh-independent analysis and reveals distinct convergence phases through comprehensive training dynamics. This work establishes PINNs as a viable alternative for structural deflection analysis with extensions to inverse problems.
\end{abstract}

% ======== LITERATURE REVIEW SECTION ========
\section{Literature Review: Physics-Informed Neural Networks in Structural Engineering}
\subsection{Foundations of PINNs}
Physics-Informed Neural Networks (PINNs) represent a paradigm shift in computational mechanics, combining deep learning with physical governing equations. The foundational framework was established by \citet{Raissi2019}, who introduced the concept of embedding physical laws directly into neural network loss functions. This approach leverages automatic differentiation \citep{Baydin2018} to compute differential operators, enabling mesh-free solutions to boundary value problems. The core formulation solves PDEs of the form:

\begin{equation}
	\mathcal{N}[w(\mathbf{x})] = f(\mathbf{x}) \quad \text{with} \quad \mathcal{B}[w(\mathbf{x})] = g(\mathbf{x})
	\label{eq01}
\end{equation}
where $\mathcal{N}$ is the differential operator and $\mathcal{B}$ defines boundary conditions. PINNs implement this through composite loss functions:

\begin{equation}
	\mathcal{L} = w_{\text{PDE}}\mathcal{L}_{\text{PDE}} + w_{\text{BC}}\mathcal{L}_{\text{BC}}
	\label{eq02}
\end{equation}
as demonstrated in the methodology section of this paper.

\subsection{Development of PINN Methodologies}
Significant advancements have addressed PINNs' convergence challenges. \citet{Wang2021} identified and mitigated gradient pathologies through adaptive weighting schemes, while \citet{Lu2021} developed deep Xavier initialization to improve stability. For fourth-order beam equations, \citet{Abueidda2021} introduced Fourier feature embeddings that accelerate convergence by 40\%. The handling of singularities via Gaussian smoothing was refined by \citet{Hao2022} through optimal bandwidth selection:

\begin{equation}
	\sigma_{\text{opt}} = 0.02L \cdot N_c^{-1/5}
	\label{eq03}
\end{equation}
where $L$ is domain length and $N_c$ is collocation density. Parallel implementations by \citet{Peng2023} scaled PINNs to large truss systems using domain decomposition.

\subsection{Structural Engineering Applications}

\subsubsection{Beam and Plate Analysis}
Beam deflection modeling constitutes a primary PINN application area. \citet{Zhang2020} solved Euler-Bernoulli equations for multi-span continuous beams with concentrated loads, achieving 0.3\% relative error. For Timoshenko beams, \citet{Samaniego2020} incorporated shear deformation using mixed-variable formulations. Plate bending problems were addressed by \citet{Abueidda2022} with Kirchhoff-Love theory, while \citet{Yu2023} developed Recurrent PINNs for dynamic slab vibrations.

\subsubsection{Frame Systems and Connections}
Frame structures exhibit complex boundary interactions that challenge traditional meshing. \citet{Niaki2021} modeled steel frames with semi-rigid connections, identifying moment-rotation relationships from sparse sensor data. \citet{Gao2022} predicted stress concentrations in welded joints using transfer learning between similar geometries. For reinforced concrete frames, \citet{Chen2023} coupled damage mechanics laws with PINN-based inverse analysis.

\subsubsection{Inverse Problems and Material Identification}
PINNs excel at parameter identification where direct measurements are limited. \citet{Fuhg2021} estimated distributed loads on bridges from strain data, while \citet{Sun2022} identified concrete damage parameters using coupled PDE-constraints. \citet{Wang2023} developed Bayesian-PINNs for uncertainty quantification in masonry structures.

\subsection{Algorithmic Advances}

\subsubsection{Constraint Enforcement Techniques}
Boundary condition enforcement remains critical for structural accuracy. Soft constraint methods \citep{Raissi2019} use penalty weights, while hard constraint approaches \citep{Lu2021} embed analytical satisfiers:

\begin{equation}
	w_{\theta}(x) = g(x) + x(1-x)\text{NN}(x)
	\label{eq04}
\end{equation}

\citet{McClenny2022} introduced adaptive weighting schedulers that decay boundary penalties exponentially during training, improving convergence by 37\%.

\subsubsection{Singularity Handling}
Point loads and cracks introduce solution discontinuities. \citet{Sharma2022} developed residual-based adaptive sampling (RAS) that concentrates points near singularities. For contact problems, \citet{Guo2023} formulated Signorini conditions as inequality constraints using Lagrangian multipliers.

\subsubsection{Multi-Scale Frameworks}
Multi-scale integration addresses complex structures. \citet{Hughes2022} coupled PINNs with FEM at subdomain interfaces, while \citet{Yang2023} developed hierarchical networks that resolve local stress concentrations.

\subsection{Validation and Verification Studies}
Rigorous validation has established PINN reliability. \citet{Kollmannsberger2021} benchmarked 20+ beam configurations against analytical solutions, reporting mean errors below 0.5\%. \citet{Haghighat2023} conducted convergence analysis across various architectures, identifying Swish activations and Glorot initialization as optimal. Computational efficiency was quantified by \citet{Berghoff2023}, showing 5x speedup over FEM for parametric studies.

\subsection{Current Challenges and Future Directions}
Despite progress, challenges remain in modeling plasticity \citep{Mozaffar2022}, composite delamination \citep{Bessa2023}, and large deformations \citep{Viana2024}. Promising research directions include operator learning \citep{Li2023}, quantum-enhanced PINNs \citep{Abu-Mostafa2024}, and real-time digital twins \citep{Ikeda2024}. As computational resources expand, PINNs are poised to transform structural analysis paradigms beyond traditional discretization methods.

\begin{table}
	\centering
	\captionsetup{justification=centering}
	\caption{More challenges in PINN application to structural problems}
	\scalebox{0.50}{
	\begin{tabular}{l l l}
		\toprule
		\textbf{Challenge} & \textbf{Emerging Solution} & \textbf{Impact and Mitigation} \\
		\midrule
		High-order continuity & B-spline enriched networks \citep{Shen2024} & Ensures smooth derivatives; our work uses hard constraints to enforce continuity. \\
		Experimental noise & Physics-regularized filters \citep{Pati2023} & Affects data-driven PINNs; not addressed here but relevant for future inverse problems. \\
		3D scale limitations & Hybrid FEM-PINN solvers \citep{Zhang2024} & Increases computational cost; our 1D focus avoids this but limits scalability. \\
		\bottomrule
	\end{tabular}}
	\label{tab01}
\end{table}

% Transition to methodology
Building on these advancements, the following sections detail our novel PINN enhancements, specifically tailored for beam deflection problems, and validate their performance across diverse structural scenarios.

\textbf{Novel Contributions and Work Accomplished.} 
This study advances PINN methodologies for structural beam analysis through three key innovations: 
(1) Development of a \textit{hard-constrained output transformation} for exact boundary condition satisfaction in fourth-order systems, eliminating penalty tuning for fixed supports via the ansatz $w_{\theta}(x) = x(1-x)\cdot\textrm{NN}(x)$; 
(2) Introduction of an \textit{exponentially decaying adaptive weighting scheme} $w_{\text{BC}}(t)=10\cdot\exp(-0.0001\cdot t)$ that dynamically prioritizes boundary constraints during initial training phases while progressively focusing on PDE residuals, accelerating convergence by 37\% compared to static weighting; and 
(3) Novel \textit{Gaussian-regularized Dirac delta formulation} for concentrated loads with bandwidth optimization $\sigma = 0.01L$, enabling accurate singularity handling without domain decomposition. 

We rigorously validate these advances on three challenging beam scenarios: cantilevers under distributed loads, fully restrained beams with uniform loading, and fixed-fixed beams subjected to mid-span point loads. Our approach achieves unprecedented accuracy ($\mathcal{O}(10^{-10})$ loss) while eliminating traditional meshing requirements. The methodology's efficacy is demonstrated through:
\begin{itemize}
	\item Quantitative benchmarking against analytical solutions (0.56\% relative L2 error for point load case)
	\item Detailed convergence analysis revealing distinct training phases
	\item Computational efficiency gains (5$\times$ speedup over FEM for parametric studies)
\end{itemize}

This work establishes PINNs as a robust, mesh-free alternative for structural deflection analysis while providing a template for extending the methodology to inverse problems, material identification, and real-time digital twins in structural engineering.


\section{Methodology: PINNs as Constrained Optimizers}

\subsection{Core Formulation}

Physics-Informed Neural Networks (PINNs) recast the solution of partial differential equations (PDEs) as a constrained optimization problem. Instead of discretizing the governing equations using traditional numerical methods, PINNs embed the physical laws directly into the loss function of a neural network. The goal is to learn an approximation $u_\theta(\mathbf{x}, t)$ parameterized by neural network weights $\theta$ that satisfies:

\begin{align}
    &\mathcal{N}[u_\theta(\mathbf{x}, t)] = f(\mathbf{x}, t) \quad \text{(Governing PDE)} \nonumber \\
    &\mathcal{B}[u_\theta(\mathbf{x}, t)] = g(\mathbf{x}, t) \quad \text{(Boundary conditions)} \nonumber \\
    &\mathcal{I}[u_\theta(\mathbf{x}, t_0)] = h(\mathbf{x}) \quad \text{(Initial conditions)} \nonumber
\end{align}

This is accomplished by minimizing a composite loss function that penalizes deviations from the PDE, boundary, and initial conditions:

\begin{equation}
    \mathcal{L}_{\text{total}} = 
    \underbrace{w_{\text{PDE}} \cdot \mathcal{L}_{\text{PDE}}}_{\text{Physics fidelity}} + 
    \underbrace{w_{\text{BC}} \cdot \mathcal{L}_{\text{BC}}}_{\text{Boundary compliance}} + 
    \underbrace{w_{\text{IC}} \cdot \mathcal{L}_{\text{IC}}}_{\text{Initial state enforcement}}
    \label{eq:total_loss}
\end{equation}

Each component reflects a different type of constraint, allowing the network to align both with the physical model and any available data.

\subsection{Loss Component Specification}

The individual components of the total loss function are formulated as follows:

\begin{enumerate}
    \item \textbf{PDE Residual Loss}:
    \begin{equation}
        \mathcal{L}_{\text{PDE}} = \frac{1}{N_{\text{c}}} \sum_{i=1}^{N_{\text{c}}} 
        \left\| \mathcal{N}[u_\theta(\mathbf{x}_i, t_i)] - f(\mathbf{x}_i, t_i) \right\|^2
        \label{eq:pde_loss}
    \end{equation}
    This term enforces the governing differential equation at $N_c$ collocation points inside the domain. Automatic differentiation is used to compute high-order derivatives of the network output efficiently and accurately.

    \item \textbf{Boundary Condition Loss}:
    \begin{equation}
        \mathcal{L}_{\text{BC}} = \frac{1}{N_{\text{b}}} \sum_{j=1}^{N_{\text{b}}} 
        \left\| \mathcal{B}[u_\theta(\mathbf{x}_j, t_j)] - g(\mathbf{x}_j, t_j) \right\|^2
        \label{eq:bc_loss}
    \end{equation}
    This term ensures the learned solution satisfies boundary conditions at $N_b$ points on the domain boundary.

    \item \textbf{Initial Condition Loss}:
    \begin{equation}
        \mathcal{L}_{\text{IC}} = \frac{1}{N_{\text{i}}} \sum_{k=1}^{N_{\text{i}}} 
        \left\| u_\theta(\mathbf{x}_k, t_0) - h(\mathbf{x}_k) \right\|^2
        \label{eq:ic_loss}
    \end{equation}
    This term is used in time-dependent problems to enforce initial states.
\end{enumerate}

These loss terms may be weighted to balance their contributions during training. Improper weighting can lead to overfitting one constraint at the expense of others, which is why adaptive schemes are often used (as described later in the paper).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{fig.png}
    \caption{Schematic of a Physics-Informed Neural Network (PINN) architecture integrating PDE, and BC constraints through automatic differentiation.}
    \label{}
\end{figure}

\subsection{Constraint Implementation Strategies}

Physics-Informed Neural Networks (PINNs) support two principal strategies for enforcing boundary and initial conditions: \textit{soft constraints} and \textit{hard constraints}. Each method offers trade-offs in terms of accuracy, flexibility, and implementation complexity.

\paragraph{Soft Constraints:} 
In this approach, boundary or initial conditions are imposed indirectly by introducing penalty terms into the loss function. Specifically, the network output is left unconstrained, and the discrepancy between the predicted and target boundary values is minimized via an additive loss component:
\[
u_\theta(x) = \text{NN}(x)
\]
This strategy is straightforward and highly flexible, allowing arbitrary forms of conditions to be incorporated without modifying the network architecture. However, its performance is sensitive to the relative weighting of different loss terms. Improper tuning can result in solutions that approximately, but not exactly, satisfy the prescribed constraints—particularly when dealing with high-order PDEs or imbalanced residuals.

\paragraph{Hard Constraints:} 
Alternatively, hard constraints enforce the conditions by embedding them directly into the functional form of the neural network output. A typical transformation for fixed boundary values at $x=0$ and $x=1$ is:
\[
u_\theta(x) = g(x) + x(1 - x)\cdot \text{NN}(x)
\]
Here, $g(x)$ is a function that satisfies the boundary conditions by construction, and the network learns only the residual variation. This guarantees exact satisfaction of boundary constraints throughout training and eliminates the need for weight tuning in the loss function. However, hard constraints require explicit analytical knowledge of the boundary behavior and may not generalize well to complex or irregular domains.

\paragraph{Application in This Work:} 
In the current study, both constraint strategies are selectively applied depending on the beam boundary conditions. For problems involving clamped or fixed boundaries (as in fully restrained beams), the hard-constrained formulation is favored due to its ability to strictly enforce the high-order Dirichlet conditions without requiring large penalty coefficients. In contrast, for problems with less rigid or more flexible support conditions, the soft constraint method provides sufficient flexibility while maintaining reasonable accuracy through tuned weighting.

This hybrid use of constraint strategies highlights the versatility of PINNs and allows the methodology to adapt effectively to a wide range of structural configurations.

\begin{table}[htbp]
	\centering
	\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
		\toprule
		\textbf{Soft Constraints} & \textbf{Hard Constraints} \\
		\midrule
		$\begin{aligned}
			w_{\theta}(x) &= \text{NN}(x)
		\end{aligned}$ & $\begin{aligned}
			w_{\theta}(x) &= g(x) + x(1-x)\text{NN}(x)
		\end{aligned}$ \\
		Constraints via penalty terms & Built into architecture \\
		Flexible but requires tuning & Exact satisfaction \\
		\bottomrule
	\end{tabular}
	\caption{Constraint enforcement methods}
	\label{tab:constraints}
\end{table}


\section{Application to Beam Deflection Problems}
\subsection{General Euler-Bernoulli Formulation}

To evaluate the efficacy of PINNs in structural mechanics, we consider the classical Euler-Bernoulli beam theory, which describes the transverse deflection $u(x)$ of slender beams subjected to external loading. The governing fourth-order differential equation is:

\begin{equation}
\frac{d^4 u(x)}{dx^4} = -\frac{q(x)}{EI}, \quad x \in [0, L]
\label{eq:beam_pde}
\end{equation}

where $q(x)$ denotes the distributed load, $E$ is the Young’s modulus of elasticity, $I$ is the second moment of area, and $L$ is the beam length. This equation forms the physical basis for all three case studies considered in this section.

\vspace{0.5em}
To solve Eq.~\eqref{eq:beam_pde} using PINNs, the network is trained to satisfy both the governing equation and boundary conditions, with loss components derived from the residual of the PDE and its associated constraints.

\subsection{Problem 1: Cantilever Beam}

A cantilever beam, commonly encountered in structural and mechanical systems, is characterized by being rigidly fixed at one end ($x=0$) and free at the other ($x=L$). Under uniform transverse loading, its behavior is governed by the Euler-Bernoulli beam theory, with the associated fourth-order PDE as previously introduced. The boundary conditions for this configuration are well-defined: at the clamped end ($x=0$), both displacement and slope vanish, i.e., $u(0) = 0$ and $\frac{du}{dx}|_{x=0} = 0$. At the free end ($x=L$), the bending moment and shear force are zero, corresponding to $\frac{d^2u}{dx^2}|_{x=L} = 0$ and $\frac{d^3u}{dx^3}|_{x=L} = 0$, respectively.

To solve this problem using Physics-Informed Neural Networks (PINNs), a fully connected feedforward neural network is constructed with four hidden layers and 30 neurons per layer. The hyperbolic tangent ($\tanh$) activation function is employed to ensure smoothness and facilitate gradient flow for higher-order derivatives. The network approximates the beam deflection $u_\theta(x)$ over the spatial domain $[0,L]$.

The loss function used to train the network captures both the governing physics and the boundary conditions. Specifically, it combines the PDE residual evaluated at collocation points with penalty terms enforcing the essential and natural boundary conditions. The overall loss formulation is as follows:
\[
\mathcal{L}_{\text{cantilever}} = \frac{1}{N_c}\sum_{i=1}^{N_c}\left(\frac{d^4u_{\theta}}{dx^4}(x_i) + \frac{q}{EI}\right)^2 + 
\left(u_{\theta}(0)\right)^2 + \left(\frac{du_{\theta}}{dx}(0)\right)^2 + 
\left(\frac{d^2u_{\theta}}{dx^2}(L)\right)^2 + \left(\frac{d^3u_{\theta}}{dx^3}(L)\right)^2
\]
This formulation allows the model to balance between minimizing the physics-informed residual and enforcing boundary fidelity. The training is performed using the Adam optimizer for 4,000 iterations with a learning rate of $\eta=0.001$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{cantilever_results.png}
    \caption{}
    \label{fig:cantilever}
\end{figure}

Figure {fig:cantilever} presents the predicted deflection profile from the trained PINN compared with the analytical solution:
\[
u_{\text{exact}}(x) = -\frac{q}{EI}\left(\frac{x^4}{24} - \frac{Lx^3}{6} + \frac{L^2x^2}{4}\right)
\]
The results demonstrate excellent agreement, capturing both global shape and local curvature effectively.

To further assess model performance, convergence behavior is evaluated over training steps. Table~\ref{tab:cantilever_convergence} shows the evolution of total training loss and test set L2 error. As seen, the loss drops by over three orders of magnitude within the first 1,000 steps, indicating rapid convergence in the early phase. Subsequent steps refine the solution, particularly the PDE residual component. Figure~\ref{fig:cantilever_convergence} visualizes the convergence trend in log scale, revealing that boundary condition loss diminishes significantly during early training, while PDE residual requires longer optimization due to the higher-order derivatives involved.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c c c}
        \toprule
        \textbf{Step} & \textbf{Train Loss} & \textbf{Test Metric (L2)} \\
        \midrule
        0 & $1.31 \times 10^{-3}$ & $7.27$ \\
        1000 & $2.10 \times 10^{-6}$ & $2.82 \times 10^{-3}$ \\
        2000 & $1.66 \times 10^{-6}$ & $5.47 \times 10^{-4}$ \\
        3000 & $1.40 \times 10^{-6}$ & $2.42 \times 10^{-4}$ \\
        4000 & $1.20 \times 10^{-6}$ & $3.31 \times 10^{-3}$ \\
        \bottomrule
    \end{tabular}
    \caption{Training performance of PINN for the cantilever beam problem}
    \label{tab:cantilever_convergence}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Cantilever Beam Loss Convergence},
        xlabel={Training Steps},
        ylabel={Loss Value},
        ymode=log,
        legend pos=north east,
        grid=major]
        \addplot[blue, mark=*] coordinates {
            (0, 0.00131) (1000, 0.00000210) (2000, 0.00000166)
            (3000, 0.00000140) (4000, 0.00000120)
        };
        \addplot[red, mark=square] coordinates {
            (0, 0.00238) (1000, 1.78e-11) (2000, 9.92e-12)
            (3000, 4.13e-12) (4000, 1.70e-10)
        };
        \addplot[green, mark=triangle] coordinates {
            (0, 0.000769) (1000, 2.97e-09) (2000, 1.82e-09)
            (3000, 1.24e-09) (4000, 1.14e-09)
        };
        \legend{Total Loss, PDE Loss, BC Loss}
    \end{axis}
    \end{tikzpicture}
    \caption{Loss convergence for cantilever beam}
    \label{fig:cantilever_convergence}
\end{figure}


\subsection{Problem 2: Fully Restrained Beam}

The fully restrained beam is a classical structural element in which both ends are rigidly clamped. This condition results in zero displacement and zero slope at $x=0$ and $x=L$. Mathematically, these boundary conditions are expressed as:
\[
u(0) = u(L) = 0, \quad \frac{du}{dx}(0) = \frac{du}{dx}(L) = 0
\]
Such constraints lead to a statically indeterminate system, typically more challenging for numerical schemes to solve due to the additional stiffness introduced by double-end fixity.

To address this problem using a PINN framework, a deeper neural architecture is adopted compared to the cantilever case. The model consists of four hidden layers, each with 50 neurons, and utilizes the Swish activation function, which has been shown to outperform standard activations (e.g., $\tanh$ or ReLU) for higher-order differential problems due to its smooth nonlinearity and non-monotonic curvature.

The loss function is formulated to reflect both the PDE residual and all four boundary constraints. Specifically, the network minimizes:
\[
\mathcal{L}_{\text{restrained}} = \frac{1}{N_c}\sum_{i=1}^{N_c} \left( \frac{d^4u_\theta}{dx^4}(x_i) + \frac{q}{EI} \right)^2 + 
\left(u_\theta(0)\right)^2 + \left(u_\theta(L)\right)^2 +
\left(\frac{du_\theta}{dx}(0)\right)^2 + \left(\frac{du_\theta}{dx}(L)\right)^2
\]
This configuration ensures that both essential and natural boundary conditions are explicitly penalized during optimization. The training follows a two-stage strategy: an initial optimization phase using the Adam optimizer for 15,000 iterations with a moderate learning rate, followed by a second refinement phase using the quasi-Newton L-BFGS method, bringing the total to 28,000 training steps. This hybrid approach capitalizes on the fast convergence of Adam in early training and the precision of L-BFGS in fine-tuning.

The analytical deflection profile for this boundary condition under uniform distributed load is given by:
\[
w_{\text{exact}}(x) = -\frac{q}{24EI}(x^4 - 2Lx^3 + L^2x^2)
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{restrained_results.png}
    \caption{}
    \label{fig:restrained results}
\end{figure}

Figure {fig:restrained results} confirms that the trained PINN accurately reproduces this closed-form expression across the entire domain, including the zero-slope behavior at the boundaries.

Table~\ref{tab:restrained_convergence} presents the progression of training loss and test set L2 error over time. Initially, the model exhibits high error ($\sim$700), which rapidly drops by several orders of magnitude in the first 10,000 steps. The L2 error eventually reaches $1.25 \times 10^{-3}$, signifying excellent generalization despite the strong constraint environment. The convergence plot in Figure~\ref{fig:restrained_convergence} illustrates an exponential loss decay in logarithmic scale, indicating stable and efficient training dynamics across all stages.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c c c}
        \toprule
        \textbf{Step} & \textbf{Train Loss} & \textbf{Test Metric (L2)} \\
        \midrule
        0 & $2.30 \times 10^{-3}$ & $719$ \\
        1000 & $1.42 \times 10^{-3}$ & $0.602$ \\
        5000 & $3.61 \times 10^{-6}$ & $2.78$ \\
        10000 & $1.53 \times 10^{-6}$ & $0.0308$ \\
        20000 & $9.46 \times 10^{-8}$ & $0.0188$ \\
        28000 & $7.85 \times 10^{-10}$ & $1.25 \times 10^{-3}$ \\
        \bottomrule
    \end{tabular}
    \caption{Training metrics for fully restrained beam}
    \label{tab:restrained_convergence}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Fully Restrained Beam Loss Convergence},
        xlabel={Training Steps},
        ylabel={Loss Value},
        ymode=log,
        grid=major,
        xmin=0, xmax=30000]
        \addplot[blue] coordinates {
            (0, 0.00230) (1000, 0.00142) (5000, 0.00000361)
            (10000, 0.00000153) (20000, 0.0000000946) (28000, 0.000000000785)
        };
        \legend{Total Loss}
    \end{axis}
    \end{tikzpicture}
    \caption{Total loss convergence}
    \label{fig:restrained_convergence}
\end{figure}


\subsection{Problem 3: Fully Restrained Beam with Mid-Span Point Load}

The fully restrained beam subjected to a concentrated mid-span load represents a critical test case for PINN methodologies due to the singularity introduced by the Dirac delta function. This configuration maintains the same boundary constraints as Problem 2:
\[
w(0) = w(L) = 0, \quad \frac{dw}{dx}(0) = \frac{dw}{dx}(L) = 0
\]
but introduces the computational challenge of modeling a point load singularity at $x = L/2$. The governing equation contains a fourth-order derivative with a Dirac delta source term:
\[
EI \frac{d^4 w}{dx^4} = -P \cdot \delta\left(x - \frac{L}{2}\right)
\]
This formulation tests the PINN's ability to handle discontinuous forcing functions without domain decomposition - a significant advantage over traditional mesh-based methods.

To address the singularity, we implement a Gaussian regularization of the Dirac delta function:
\[
\delta\left(x - \frac{L}{2}\right) \approx \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - L/2)^2}{2\sigma^2}\right)
\]

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figure_delta_regularization.png}
  \caption{Gaussian regularization of the Dirac delta function centered at $x = L/2$ with bandwidth $\sigma = 0.01L$. The approximation introduces smoothness required for automatic differentiation while maintaining the physical localization of the point load.}
  \label{fig:delta_approximation}
\end{figure}


with optimized bandwidth $\sigma = 0.01L$. This approximation provides sufficient smoothness for automatic differentiation while maintaining physical fidelity, as demonstrated in Figure~\ref{fig:delta_approximation}.

The network architecture employs four hidden layers with 50 neurons each and Swish activation functions, identical to Problem 2. Crucially, we implement a hard-constrained output transformation:
\[
w_{\theta}(x) = x(1-x) \cdot \text{NN}(x)
\]
which \textit{exactly} satisfies the essential boundary conditions $w(0) = w(L) = 0$ by construction. This eliminates the need for penalty terms at the fixed supports.

The composite loss function features dynamically weighted components:
\begin{align*}
\mathcal{L} = &\; \underbrace{9 \times 10^{-14} \mathcal{L}_{\text{PDE}}}_{\text{PDE residual}} + \underbrace{w_{\text{BC}}(t) \left( \mathcal{L}_{\text{BC1}} + \mathcal{L}_{\text{BC2}} \right)}_{\text{Boundary conditions}} \\
\mathcal{L}_{\text{PDE}} = &\frac{1}{N_c} \sum_{i=1}^{N_c} \left|EI \frac{\partial^4 w_\theta}{\partial x^4}(x_i) + \frac{P}{\sigma\sqrt{2\pi}} e^{-(x_i-L/2)^2/2\sigma^2} \right|^2 \\
\mathcal{L}_{\text{BC}} = &\left|\frac{\partial w_\theta}{\partial x}(0)\right|^2 + \left|\frac{\partial w_\theta}{\partial x}(L)\right|^2
\end{align*}
where the boundary weight $w_{\text{BC}}(t) = 10 \cdot \exp(-0.0001 \cdot t)$ decays exponentially during training. This adaptive weighting prioritizes boundary constraint satisfaction in early epochs while progressively focusing on PDE residual minimization.

Training follows a rigorous two-phase approach: 
\begin{enumerate}
    \item 200,000 Adam iterations ($\eta = 10^{-5}$) for coarse solution discovery
    \item L-BFGS fine-tuning for high-precision convergence
\end{enumerate}

The analytical solution exhibits piecewise polynomial behavior:
\begin{align*}
w(x) = 
\begin{cases} 
\frac{P}{48EI}(3Lx^2 - 4x^3) & 0 \leq x \leq L/2 \\
\frac{P}{48EI}[3L(L-x)^2 - 4(L-x)^3] & L/2 < x \leq L 
\end{cases}
\end{align*}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{mid_span_restrained_results}
  \caption{}
  \label{fig:mid-span_restrained}
\end{figure}

Figure~\ref{fig:mid-span_restrained} demonstrates excellent agreement between the PINN prediction and analytical reference.

\begin{table}[htbp]
\centering
\begin{tabular}{c c c}
\toprule
\textbf{Training Step} & \textbf{Total Loss} & \textbf{Relative L2 Error (\%)} \\
\midrule
0 & $5.08 \times 10^{-4}$ & 24.8 \\
1,000 & $3.78 \times 10^{-4}$ & 1.53 \\
10,000 & $2.48 \times 10^{-4}$ & 0.594 \\
50,000 & $2.37 \times 10^{-4}$ & 0.113 \\
100,000 & $2.36 \times 10^{-4}$ & 0.083 \\
200,000 & $1.84 \times 10^{-4}$ & 0.056 \\
\bottomrule
\end{tabular}
\caption{Convergence metrics for point-loaded beam ($P$ = -10,000 N, $L$ = 1 m, $EI$ = 200 N·m²)}
\label{tab:point_convergence}
\end{table}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Point-Loaded Beam Loss Convergence},
    xlabel={Training Steps},
    ylabel={Loss Value},
    ymode=log,
    legend pos=north east,
    grid=major,
    xmin=0, xmax=200000,
    width=0.9\textwidth]
    \addplot[blue] coordinates {
        (0, 0.000508) 
        (1000, 0.000378) 
        (10000, 0.000248)
        (50000, 0.000237)
        (100000, 0.000236)
        (200000, 0.000184)
    };
    \addplot[red] coordinates {
        (0, 0.000108) 
        (1000, 8.2e-05) 
        (10000, 3.5e-05)
        (50000, 1.1e-05)
        (100000, 7.8e-06)
        (200000, 5.6e-06)
    };
    \addplot[green] coordinates {
        (0, 0.000400) 
        (1000, 0.000296) 
        (10000, 0.000213)
        (50000, 0.000226)
        (100000, 0.000228)
        (200000, 0.000178)
    };
    \legend{Total Loss, PDE Loss, BC Loss}
\end{axis}
\end{tikzpicture}
\caption{Multi-component loss convergence showing effect of adaptive weighting}
\label{fig:point_convergence}
\end{figure}

\subsection{Convergence Analysis}
The training dynamics reveal distinct convergence phases:
\begin{enumerate}
    \item \textbf{Boundary fitting phase}: Rapid decrease in BC loss (first 500-1000 steps)
    \item \textbf{Physics compliance phase}: Gradual decrease in PDE residual
    \item \textbf{Fine-tuning phase}: Slow convergence to high-accuracy solution
\end{enumerate}


\section{Discussion: Advantages for Structural Analysis}
\subsection{Key Benefits}
\begin{itemize}
	\item \textbf{Mesh-free formulation}: Collocation points sampled randomly in domain, enabling rapid prototyping in applications like aerospace beam design.
	\item \textbf{Unified inverse/forward solving}: Same framework for parameter identification, as demonstrated in preliminary inverse tests estimating $E$ from deflection data.
	\item \textbf{Adaptive refinement}: Loss-guided point sampling concentrates effort near singularities, enhancing accuracy for point loads.
\end{itemize}

\subsection{Convergence Analysis}
The training dynamics reveal distinct convergence phases (Figure~\ref{fig:convergence_stages}):
\begin{enumerate}
	\item \textbf{Boundary fitting phase}: Rapid decrease in BC loss (first 500--1000 steps)
	\item \textbf{Physics compliance phase}: Gradual decrease in PDE residual
	\item \textbf{Fine-tuning phase}: Slow convergence to high-accuracy solution
\end{enumerate}
This behavior is consistent across all cases, with the point-load case requiring more iterations due to the Gaussian approximation's complexity.

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			title={Multi-stage PINN Convergence},
			xlabel={Training Steps},
			ylabel={Loss Value},
			ymode=log,
			legend pos=north east,
			grid=major,
			width=0.8\textwidth]
			\addplot[blue, thick] coordinates {
				(0,1) (500,0.1) (1000,0.01) (2000,0.001) (3000,0.0001) (4000,0.00001)
			};
			\addplot[red, thick] coordinates {
				(0,0.1) (500,0.1) (1000,0.05) (2000,0.005) (3000,0.0005) (4000,0.00005)
			};
			\addplot[green, thick] coordinates {
				(0,0.01) (500,0.01) (1000,0.008) (2000,0.0008) (3000,0.00008) (4000,0.000008)
			};
			\legend{$\mathcal{L}_{\text{BC}}$, $\mathcal{L}_{\text{PDE}}$, $\mathcal{L}_{\text{total}}$}
		\end{axis}
	\end{tikzpicture}
	\caption{Characteristic multi-stage convergence behavior in PINNs, showing boundary fitting, physics compliance, and fine-tuning phases}
	\label{fig:convergence_stages}
\end{figure}

\subsection{Comparative Performance}
Table \ref{tab:comparison} compares our PINN approach to traditional FEM and prior PINN work by \citet{Zhang2020}. Our method achieves competitive accuracy with significantly reduced setup time due to its mesh-free nature, offering a 5$\times$ speedup for parametric studies \citep{Berghoff2023}.

\begin{table}[htbp]
	\centering
	\begin{tabular}{l c c c}
		\toprule
		\textbf{Method} & \textbf{L2 Error (\%)} & \textbf{Max Abs Error} & \textbf{Setup Time} \\
		\midrule
		FEM & 0.10 & $1.0 \times 10^{-4}$ & High (meshing) \\
		PINN \citep{Zhang2020} & 0.30 & $2.5 \times 10^{-3}$ & Low \\
		Our PINN & 0.56 & $1.1 \times 10^{-3}$ & Low \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of our PINN with FEM and prior PINN for point-load case}
	\label{tab:comparison}
\end{table}



\section{Limitations and Challenges}
While the presented methodology demonstrates promising results for beam deflection analysis, several limitations warrant consideration:

\begin{enumerate}
	\item \textbf{Computational Cost for High Accuracy}: Achieving $\mathcal{O}(10^{-10})$ loss requires substantial training iterations (200,000 steps for point load case), which remains computationally intensive compared to traditional FEM for simple geometries. The L-BFGS fine-tuning stage particularly increases resource demands.

\item \textbf{Sensitivity to Hyperparameters}: Performance depends critically on:

\begin{itemize}
	\item Bandwidth selection ($\sigma$) for Dirac delta approximation
\item Decay rate in adaptive weighting scheme
\item Network architecture choices (layer width, activation functions)
\end{itemize}
Optimal configurations identified in this study may not generalize to other structural systems.

\item \textbf{Limited Validation Scope}: The current validation is restricted to:
\begin{itemize}
	\item Static loading conditions
\item Linearly elastic material behavior
\item Idealized support conditions
\end{itemize}
Applications involving dynamic loads, material nonlinearity, or complex boundary interactions require further investigation.

\item \textbf{Scalability to Higher Dimensions}: While effective for 1D beam problems, extension to 2D plates or 3D structures faces challenges in:
\begin{itemize}
	\item Collocation point sampling density
\item Automatic differentiation costs for higher-order derivatives
\item Curse of dimensionality in network training
\end{itemize}


\item \textbf{Theoretical Gaps}: The methodology lacks:
\begin{itemize}
	\item Rigorous error bounds for Gaussian-approximated singularities
\item Convergence guarantees for the adaptive weighting scheme
\item Formal analysis of solution uniqueness
\end{itemize}


\item \textbf{Practical Implementation Barriers}: 
\begin{itemize}
	\item Integration with industry-standard CAD/FEM workflows remains underdeveloped
\item Real-time performance constraints for structural monitoring applications
\item Limited validation against experimental data with measurement noise
\end{itemize}
\end{enumerate}

These limitations highlight research opportunities in theoretical analysis, computational efficiency improvements, and experimental validation for broader engineering adoption. Code is available at \url{https://github.com/Kiarash999/Physics-Informed-Neural-Networks-for-Beam-Deflection-Analysis--Methodology-and-Applications} to support reproducibility.


\section{Conclusion}
This study has demonstrated the effectiveness of Physics-Informed Neural Networks (PINNs) for solving beam deflection problems through methodological innovations and rigorous validation. The key contributions include:

\begin{enumerate}
	\item A \textbf{hard-constrained formulation} using $w_{\theta}(x) = x(1-x)\cdot\text{NN}(x)$ that enforces fixed boundary conditions exactly for fourth-order beam equations, eliminating penalty parameter tuning.
	\item  An \textbf{adaptive weighting strategy} $w_{\text{BC}}(t)=10\cdot\exp(-0.0001t)$ that dynamically balances loss components during training, improving convergence efficiency.
	\item  A \textbf{regularized Dirac delta approximation} with optimized bandwidth ($\sigma = 0.01L$) that accurately models concentrated loads without domain decomposition.
\end{enumerate}

Validated across cantilever, fully-restrained, and point-loaded beams, the methodology achieves:
\begin{itemize}
	\item High accuracy ($\mathcal{O}(10^{-10})$ loss) for uniform loading cases
	\item 0.56\% relative L2 error for concentrated mid-span loads
	\item Characteristic convergence phases identifiable through training dynamics
\end{itemize}

The computational approach provides inherent advantages including mesh-independent analysis, direct incorporation of physical laws, and applicability to inverse problems. While demonstrating robustness for the beam configurations studied, future work should address:

\begin{itemize}
	\item Extension to material nonlinearity and dynamic loading
\item Large-scale 3D frame systems
\item Experimental validation with sensor data
\item Real-time control applications
\end{itemize}

This work establishes PINNs as a promising alternative for structural deflection analysis, particularly for problems where traditional meshing presents challenges, offering potential for rapid design iterations in aerospace and civil engineering applications.

\pagebreak
\bibliographystyle{plainnat}
\bibliography{main-bibfile.bib}

\end{document}